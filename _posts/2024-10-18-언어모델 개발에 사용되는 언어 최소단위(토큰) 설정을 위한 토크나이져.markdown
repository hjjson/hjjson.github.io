---
layout: post
title:  "언어모델 개발에 사용되는 언어 최소단위(토큰) 설정을 위한 토크나이져"
date:   2024-10-18
categories: 토크나이져 형태소분석기 단어사전
---


배경
- 현재 소비자원 프로젝트의 일환으로 상호, 상품, 주소, 이름 등을 포함한 비식별화 ner 테거를 개발하고 있으며, ner 테거에서 문장 토큰화는 필수적인 과정임
- NER 테거 개발은 회사의 오랜 과제이고, 적절한 핑계가 있을 때 조금이라도 이바지하기 위하여 본 주제를 설정함

본론
1. AI 모델 학습하기 위해 단어 사전의 개수가 중요한 이유
  데이터 : [“서울”, “부산”, “대구”, “인천”, “광주”, “대전”, “울산”]‘
  - 위 데이터를 학습시켜 첫 글자로 다음 글자를 생성하는 언어모델을 만든다고 가정함
단어사전 : {'광': 0, '구': 1, '대': 2, '부': 3, '산': 4, '서': 5, '울': 6, '인': 7, '전': 8, '주': 9, '천': 10}
  - 우선, 글자 한 개를 토큰 단위로 하는 단어사전을 위와 같이 만들고, 문자를 숫자로 전화하기 위한 기준으로 사용함
  - “대” 라는 글자가 입력값으로 들어오면 단어사전을 이용하여 “대”를 숫자 2로 전환함
  - 숫자 2를 원핫벡터로 전환 후 embedding layer에 들어감
  - AI 모델을 통과한 결과가 sigmoid 함수와 같은 decision layer를 통과하여 다음 글자를 상징하는 원핫벡터로 변환됨
  - 마지막으로 decision layer를 통과한 원핫벡터가 단어사전을 이용하여 원래 글자로 복원됨.
  - 위 과정을 통해 언어모델이란 10개의 벡터값을 기준으로 10개 중 하나의 정답을 맞추는 문제가 됨을 이해할 수 있음.
  - 즉 단어사전에 단어 개수가 많아질수록 문제가 어려워 짐을 알 수 있음

2. 적정 언어 최소 단위(토큰) 크기 설정
  - 데이터가 많다는 가정하에 토큰 단위가 짧을수록 단어사전의 단어 개수가 적어지고, 토큰 단위가 길면 단어사전의 단어 개수가 많아짐.
  - 예를 들어, ["대구를", "대구는", "대구가", "대구다", "대구다.", "구미를", "구미는",  "구미가", "구미다", "구미다."] 데이터는 같은 단어가 하나도 없는 상태이며, 각각을 토큰으로 만든다고 가정하면 총 10개의 단어 개수가 나오지만, 문자 하나를 토큰으로 만들면  {'.': 0, '가': 1, '구': 2, '는': 3, '다': 4, '대': 5, '를': 6, '미': 7} 와 같이 총 8개의 단어 개수가 만들어 짐
  - 단, “1 AI 모델 학습하기 위해 단어 사전의 개수가 중요한 이유” 내용을 근거로 단어사전의 단어 개수가 적어지면 문제가 쉬워져서 문자 단위의 토큰을 적용하면 문제 대부분이 해결될 것같이 보이지만, AI는 입력데이터의 의미를 정답추론의 근거로 사용한다는 점과 입출력 길이가 달라진다는 점에서 단어사전의 크기를 무작정 적게 만드는 것만으로 문제 해결 방법이 되지 않음
  
  - 오른쪽 그림에서 보는 것처럼 “대구에 관하여”를 번역한다면 입력값의 길이가 달라짐 
  - 출력값도 똑같이 문자를 토큰으로 하면 (1/n) * 10의 문제를 해결해야 하는 AI이지만, 단어 단위로 나누면 (1/n) *2를 해결해야 하는 문제가 됨
  - 결과적으로 적절한 토큰 크기와 그에 따른 단어사전 크기를 설정하기 위한 다양한 방법이 도입되고 있음

3. 언어 최소 단위(토큰) 설정
  1) 단순 띄어쓰기
    - “서울에 사는 김철수는 직장인입니다” -> [“서울에”, “사는”, “김철수는”, “직장인입니다”] 와 같이 토큰을 제작하는 방법
    - [“서울에”, “서울은”, “서울의” , “서울에서”] 등이 다 다른 토큰으로 간주하는 문제점이 있어서 많이 사용되지 않음
    - 잘못 작성한 문장에 대한 처리 방법이 없음. 예를 들어 “서울에사는김철수는 직장인입니다”와 같은 문장이 [“서울에사는김철수는”, “직장인입니다”]와 같이 토큰화됨
  2) 형태소 분석기
    - “서울에 사는 김철수는 직장인입니다” -> [“서울”, 에”, “사는”, “김철수”, ”는”, “직장인”, “입니다”] 와 같이 토큰을 제작하는 방법
    - 형태소 분석기 성능에 따라 차이는 있겠지만 명사 구분에서 문제점을 보임. 예를 들어, 기프티콘과 같은 회사 이름을 [“기”, “프티”, “콘”]과 같이 구분 시킴.
    - (참고 : AI 모델은 파라미터 수가 많은 만큼 데이터가 많아야 성능을 냄, 파라미터가 많으면 입력값이 커도 성능이 나옴) 소비자원 프로젝트같이 짧은 시간에 라벨링을 많이 할 수 없는 경우 학습에 사용할 데이터가 많지 않음
    - 형태소 분석기를 사용하면 주로 단어 단위로 토큰이 만들어지기 때문에 의미 전달은 가능하지만 토큰 개수가 지나치게 많아짐. 소비자원에 사용될 데이터 모두를 형태소 분석기를 돌리고 unique 단어를 선택하면 약 10만 개 이상의 토큰이 만들어짐 
  3) 글자 단위
    - “서울에 사는 김철수는 직장인입니다” -> [“서”, “울”, “에”, “_사”, “는”, “_김”, “철”, “수”, ”는”, “_직”, “장”, “인”, “입”, “니”, “다”] 와 같이 토큰을 제작하는 방법
    - 소비자원 데이터를 대상으로 글자 단위 토큰을 만들 때 토큰이 약 2천 개 만들어질 정도로 단어사전 크기가 줄어들고 이는 입력데이터의 의미 반영 면에서 좋지 않음
    - 위 이유로 상용 토크나이져는 앞에 띄어쓰기가 있는 토큰과 아닌 토큰을 다른 토큰으로 처리함. 예를 들어, “입력”이라는 단어에 “입”은 띄어쓰기와 함께 “_입” 토큰으로 처리되고 “입니다”라는 단어는 “입” 토큰으로 처리함


결론
- 소비자원 과제는 글자 단위 및 대표적인 명사 일부만이 토큰 단위로 사용되는 상용 토크나이져를 이용하고 있으며, 라벨링 결과에 한 번 더 토크나이져를 돌리고 있음
- 글자 단위를 기본으로 대표적인 명사를 늘리는 방안을 고민할 필요성이 있음.
- 모델 변경 혹은, 토크나이져 변경에 따른 성능의 변화를 확인해 볼 필요성이 있음